# 4.神经网络学习导论：梯度下降

## 4.2 比较

### 比较

- Definition

  比较是为预测的“误差”提供度量。

- Purpose for measure error

  1. 测量误差为简化问题。因为现在的问题就是怎么让误差变为零。
  2. 在测量误差的不同方法中，误差的优先级不同。使用均方误差，能够使得较大的误差被优先考虑。
  3. 误差总是正的。因为如果一个误差是1000，另一个是-1000，平均误差是零，但事实是每次预测的误差是1000。
  
- Calculate

  ```python
  error = ((input * weight) - goal_prediction) ** 2
  # 解析
  # 目的是使得error最小，找到局部最优解，那就是调整公式中的变量的值
  # 调整input：更改输入数据，属于创造值，不合理
  # 调整goal_prediction：修改现实数据，不合理
  # 调整2：仅仅更改了计算误差的方式
  # 调整weight：不会改变目标，不会改变现实数据，不会破坏测量方法
  ```


## 4.3 冷热学习

### 学习

- Definition：为了表明权重应该如何改变来降低误差。

### 冷热学习

- Definition：扰动权重来确定向哪个方向调整可以使得误差的降幅最大，并不断移动，直到误差趋紧于0。

- Purpose：调整权重，以降低误差。

- Implementation

  <!--单次的实现-->

  ```python
  import numpy as np
  
  weight = 0.1
  lr = 0.01  # gradient descent
  
  
  # Calculate predicted value
  def neural_network(input, weight):
      output = input * weight
      return output
  
  
  # Calculate error
  def calculate_error(pred, true):
      error = (pred - true) ** 2
      return error
  
  
  # Cold and hot
  def gradient_descent(lr, input, weight, true):
      pred = neural_network(input, weight)
      error = calculate_error(pred, true)
      # The first descent
      pred_down = neural_network(input, (weight - lr))
      error_down = calculate_error(pred_down,true)
      pred_up = neural_network(input, (weight + lr))
      error_up = calculate_error(pred_up, true)
      
      # The second descent
      if error > error_down or error > error_up:
          if(error_up > error_down):
              weight -= lr
              pred_down = neural_network(input, (weight - lr))
              error = calculate_error(pred_down,true)
          if(error_down > error_up):
              weight += lr
              pred_up = neural_network(input, (weight + lr))
              error = calculate_error(pred_up, true)
      return error
  
  
  number_of_toes = np.array([8.5])
  win_or_lose_binary = np.array([1])  # won
  
  input = number_of_toes[0]
  true = win_or_lose_binary[0]
  
  error = gradient_descent(lr, input, weight, true)
  print(error)
  ```

  ```shell
  # Analyse
  1. 思路
  	1. 目的是计算出最优的Error值
  	2. 编写计算预测的方法
  	3. 编写计算预测误差的方法
  	4. 编写计算修改步伐后的误差的方法：判断原误差，下降一个lr，上升一个lr之间的大小，最小的则为下一次下降的方法，再进行一次下降
  ```

  <!--循环多次的实现-->

  ```python
  import numpy as np
  
  weight = 0.5
  input = 0.5
  goal_predition = 0.8
  
  step_amount = 0.001
  
  for iteraction in range(1101):
      predition = input * weight
      error = (predition - goal_predition) ** 2
      print("Error:" + str(error) + "     " + "Prediction:" + str(predition))
  
      # Up descent
      up_prediction = input * (weight + step_amount)
      up_error = (up_prediction - goal_predition) ** 2
      # Down descent
      down_prediction = input * (weight - step_amount)
      down_error = (down_prediction - goal_predition) ** 2
  
      # Check the direct to descent
      if up_error > down_error:
          weight -= step_amount
      if up_error < down_error:
          weight += step_amount
  ```

  ```shell
  # Analyse
  1. 相比于单次的实现：添加了循环，删除了方法
  ```

## 4.9 梯度下降法

- Definition：将权重值往梯度（导数）值的相反方向移动，可以使error趋向于0。
- Purpose：找到正确的方向和幅度来改变权重，使误差减小。

> Q：为什么不让损失函数求偏导直接等于零来求最优解
>
> A：因为神经网络的损失函数（基本都是非凸函数）大多都没有解析解，只能寻找近似解

- Implementation

  **<!--权重调节的方向和数量-->**

  **important!!!**

  ```python
  import numpy as np
  weight = 0.5
  goal_pred = 0.8
  input = 0.5
  
  for iteration in range(20):
      pred = input * weight
      error = (pred - goal_pred) ** 2
      direction_and_amount = (pred - goal_pred) * input # important!!!!
      weight = weight - direction_and_amount
  
      print("Error:" + str(error) + "     " +"Prediction:" + str(pred))
  ```

  ```shell
  # Analyse
  1. 关键是理解含义：direction_and_amount = (pred - goal_pred) * input
  	1. pred，goal_pred，input三个属性的目的是将纯误差转换为需要的权重调节的绝对幅度；第一项：(pred - goal_pred) 是纯误差（预测值和真实值的差值），表示当前error的原始方向和幅度；第二项：input与第一项纯误差相乘，用于执行缩放，负值反转和停止调节。
  	2. 停止调节：假设input为零，则误差Error不会改变，因此没有调节的意义，所以input为零时就停止权重调节。
  	3. 负值反转：如果输入值为正，向上调节权重的值能够使预测结果也向上提升（因为pred = input * (weight + step_amount)）；但是如果输入值为负，则向上调节权重后使预测结果下降；因此需要第一项需要乘以input，再次把值反转过来。
  	4. 缩放：将通过Alpha处理
  ```

  <!--梯度下降的一次迭代-->

  ```python
  import numpy as np
  
  weight = 0.1
  alpha = 0.01
  
  
  # Neural network code
  def neural_network(input, weight):
      output = input * weight
      return output
  
  # 训练集
  number_of_toes = [8.5]
  win_or_lose_binary = [1]
  
  input = number_of_toes[0]
  goal_prediction = win_or_lose_binary[0]
  
  # 预测值和误差
  pred = neural_network(input, weight)
  error = (pred - goal_prediction) ** 2
  
  # delta,纯误差
  delta = pred - goal_prediction
  # weight_delta,度量权重所导致的网络犯错的指标——即direction_and_amount
  weight_delta = delta * input
  # 控制网络的学习速度，也就是下降速度
  weight -= weight_delta * alpha
  ```

  ```shell
  # Analyse
  1. 与上一个实现相比较，将direction_and_amount细分成delta，weight_delta，并添加alpha变量表示学习速度。
  ```

### 误差和权重的关系推导

- **推导过程——important!!!**
  1. 误差和权重的关系为：error = ((input * weight) - goal_prediction) ** 2
  2. 因为关系是确定的，其中input是输入数据，goal_prediction是实际数据，修改他们不合理
  3. 因此该等式能够确切定义误差（Error）和权重（Weight）的关系
  4. 因此如果要知道误差如何随着权重的变化而变化，则对该等式对权重进行求导，所得的就是权重的变化引起的误差的变化（也就是两个变量之间的影响的敏感度）。
  5. 误差增量：Weight_delta = (input * weight - goal_prediction) * input，就是导数。

- Implementation

  ```python
  import numpy as np
  weight, goal_prediction, input = (0.0, 0.8, 1.1)
  
  for iteration in range(4):
      print("------Weight:" + str(weight))
      pred = input * weight
      error = (pred - goal_prediction) ** 2
      delta = pred - goal_prediction
      weight_delta = delta * input
      weight = weight - weight_delta
      print("Error:" + str(error) + "     " + "Prediction:" + str(pred))
      print("Delta:" + str(delta) + "     " + "Weight_delta:" + str(weight_delta))
  ```

  ```shell
  # output
  ------Weight:0.0
  Error:0.6400000000000001     Prediction:0.0
  Delta:-0.8     Weight_delta:-0.8800000000000001
  ------Weight:0.8800000000000001
  Error:0.02822400000000005     Prediction:0.9680000000000002
  Delta:0.16800000000000015     Weight_delta:0.1848000000000002
  ------Weight:0.6951999999999999
  Error:0.0012446784000000064     Prediction:0.76472
  Delta:-0.03528000000000009     Weight_delta:-0.0388080000000001
  ------Weight:0.734008
  Error:5.4890317439999896e-05     Prediction:0.8074088
  Delta:0.007408799999999993     Weight_delta:0.008149679999999992
  ```

  - 解析
    - 第一条，当Weight_delta的值（导数）为负的时候，第二条的weight向正向移动（也就是增加）到0.88
    - 第二条，当Weight_delta的值（导数）为正的时候，第三条的weight向负向移动（也就是减少）到0.695
    - 验证了权重向梯度（导数）值相反的方法运动，来实现error趋向于0

### 过度修正及解决方案

- 发散

  - Definition：由于过度修正而导致发散

  - Implementation

    <!--破坏梯度下降-->

    ```python
    import numpy as np
    weight, goal_prediction, input = (0.5, 0.8, 2)
    
    for iteration in range(4):
        pred = input * weight
        error = (pred - goal_prediction) ** 2
        delta = pred - goal_prediction
        weight_delta = delta * input
        weight = weight - weight_delta
        print("Error: " + str(error) + "     " + "Prediction: " + str(pred))
    ```

    ```shell
    # output
    Error: 0.03999999999999998     Prediction: 1.0
    Error: 0.3599999999999998     Prediction: 0.20000000000000018
    Error: 3.2399999999999984     Prediction: 2.5999999999999996
    Error: 29.159999999999986     Prediction: -4.599999999999999
    ```

    ```shell
    # Analyse
    1. Error的值越来越大，说明越来越远离目标（目标是使得error趋向于零）。
    2. 原因是input过大，造成weight_delta过大，导致weight变动过大，造成过度修正，引起发散。
    ```

- **解决方法——important!!!**

  - 理论：引入α来防止过度修正。一般取值在（10，1，0.1，0.01，0.001，0.0001），通过测试来确定具体使用哪个值。

  - Implementation

    ```python
    import numpy as np
    weight, goal_prediction, input = (0.5, 0.8, 2)
    alpha = 0.1
    for iteration in range(20):
        pred = input * weight
        error = (pred - goal_prediction) ** 2
        delta = pred - goal_prediction
        weight_delta = delta * input
        weight = weight - alpha * weight_delta
        print("Error:" + str(error) + "    " + "Prediction:" + str(pred))
    ```

    ```shell
    # output
    Error:0.03999999999999998    Prediction:1.0
    Error:0.0144    Prediction:0.92
    Error:0.005183999999999993    Prediction:0.872
    Error:0.0018662400000000014    Prediction:0.8432000000000001
    Error:0.0006718464000000028    Prediction:0.8259200000000001
    Error:0.00024186470400000033    Prediction:0.815552
    Error:8.70712934399997e-05    Prediction:0.8093312
    Error:3.134566563839939e-05    Prediction:0.80559872
    Error:1.1284439629823931e-05    Prediction:0.803359232
    Error:4.062398266736526e-06    Prediction:0.8020155392
    ```

    ```shell
    # Analyse
    1. 通过添加alpha属性来防止过度修正。
    ```

