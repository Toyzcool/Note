# 6.第一个深度神经网络：反向传播

## 6.3 矩阵和矩阵关系

- Rules

  - 惯例是用一行来表示每个样例记录，每一列表示每一项（属性）

    <!--比如第一列是脚趾头数量，第二列是胜负记录，第三列是粉丝数量。而每一行都是这三个属性的一个样例记录-->

- Implementation

  <!--学习交通信号灯停止和通行-->

  ```python
  import numpy as np
  
  weights = np.array([0.5, 0.48, -0.7])
  alpha = 0.1
  
  # Training data
  streetlights = np.array([
      [1, 0, 1],
      [0, 1, 1],
      [0, 0, 1],
      [1, 1, 1],
      [0, 1, 1],
      [1, 0, 1]])
  walk_vs_stop = np.array([0, 1, 0, 1, 1, 0])
  
  # Training
  for iter in range(40):
      error_for_all_lights = 0
      for row_index in range(len(walk_vs_stop)):
          input = streetlights[row_index]
          goal_prediction = walk_vs_stop[row_index]
  
          prediction = input.dot(weights)
  
          error = (prediction - goal_prediction) ** 2
          error_for_all_lights += error
  
          delta = prediction - goal_prediction
          weights = weights - (alpha * (input * delta))
          print("Prediction:" + str(prediction))
      print("Error:" + str(error_for_all_lights) + "\n")
  ```

  ```shell
  # Analyse
  1.思路
  	1.设置权重和学习率
  	2.输入训练数据，包括总灯上的三种信号灯分别是亮或暗属性，以及停止还是通行的记录
  	3.设计第一层循环，用于设置训练次数；用变量存储总error值
  	4.设计第二层循环
  	5.给input和goal_prediction赋值
  	6.通过input和weights求的预测的值
  	7.计算erroe，并累加总error
  	8.调整weights
  	9.重新开始循环
  ```

  ```shell
  # output
  Prediction:-0.0022410273814405524
  Prediction:0.9978745386023716
  Prediction:-0.016721264429884947
  Prediction:1.0151127459893812
  Prediction:0.9969492081270097
  Prediction:-0.0026256193329783125
  Error:0.00053373677328488
  ```

> 感觉神经网络，就是聚类和分类，计算属于某个类的可能性，来最小化误差。用来计算两个数据集之间的相关性。

### 向上和向下的压力

- 向上的压力是向1的压力，用+表示，表示相关性；向下则相反，表示无关性。

  <!--如Implementation中的第一个样例，输入的属性分别是[1,0,1]。结果prediction是1，goal_prediction是0，这会导致error值大，导致delta值大，从而weight_delta的值大，导致weights减小。但是由于中间的属性值为0，因此weights减少的是针对左右两个属性。也就是说第一个样例对weights中的左右两个值产生了向下的压力。-->

## 6.7 完全、批量和随机梯度下降

### 随机梯度下降

- Definition：针对每个执行样例执行预测和更新权重。

  <!--和6.3中的implementation一样-->

### 完全梯度下降

- Definition：针对整个训练集计算weight_delta的平均值，使用平均值来更新权重。

  <!--不同于6.3中的implementation-->

### 批量梯度下降

- Definition：自定义批次大小的样例，再更新权重。

  <!--比如，选择8个样例之后更新一次权重-->

## 6.10 边界情况

### 过拟合

- Definition：某个特定权重出现预测数据集和输出数据集完美吻合（error为零），此时并未给真正重要的输入赋予最大权重。

  <!--深度学习的最大弱点-->

### 压力冲突

Default

## 6.14 堆叠神经网络

### 间接相关性

- Definition：如果输出和输出之间没有相关性，则再创建一个神经网络。第一个神经网络将创建一个与输出具有有限相关性的中间数据集，第二个将使用这种有限相关性正确预测输出。

### 反向传播

- Definition：是反向传递增量信号的过程，通过当前层的增量（delta = goal_prediction - predition）乘以上一层各个权重，来计算出上一层的增量。

  <!--比如通过layer1预测layer2，layer1至layer2的权重描述了layer1至layer2的预测的贡献份额，同时也描述了layer1至layer2的误差的贡献份额-->

- Purpose

  - **计算权重：权重调整量 = 输出增量 * 输入值——important!!!**

    <!--比如layer1和layer2。layer2是输出量，输出增量需要是+0.25。layer1的权重是0.5，即输入值。因此权重的调整量为0.25 * 0.5 = 0.125-->

### 线性与非线性

- Rules
  1. 任何一个三层的神经网络，都存在着一个有着相同行为模式的两层神经网络。因此如果没有“非线性”技术（选择性相关），则两层和三层神经网络没有区别。
  2. 既然如规则1所述，三层神经网络的目的是什么？答案见 <u>选择相关</u>

### 选择相关

- Definition：中间层能够控制和输入层的相关性，类似于一个相关性的开关。——important!!!

  <!--比如，中间层想和某个输入节点相关联时才关联，其他情况下都不关联。在已知输出层和输出层不存在相关性的情况下，中间层能够选择性的和输入节点相关。-->

- Rules

  - 通过在任何中间节点为负时关闭它，能够允许神经网络选择性的接受来自不同输入的相关性。——也称为“非线性”

> 通过计算误差和任何一项权重的关系，知晓误差和权重的具体关系，然后将误差减少到0。最终目标就是寻找输入层和输出层之间的相关性。如果不存在相关性，则误差永远不会达到0。

### Implementation

```python
import numpy as np

np.random.seed(1)


# Function: 将所有负数设置为零
def relu(x):
    return (x > 0) * x


# Function: 当input大于0时，返回1；否则返回0
def relu2deriv(input):
    return input > 0


alpha = 0.2
hidden_size = 4

# Training data
streetlights = np.array([
    [1, 0, 1],
    [0, 1, 1],
    [0, 0, 1],
    [1, 1, 1]
])
walk_vs_stop = np.array([1, 1, 0, 0]).T

# Weights，随机生成两组权重，分别两层
weights_0_1 = 2 * np.random.random((3, hidden_size)) - 1
weights_1_2 = 2 * np.random.random((hidden_size, 1)) - 1

# Begin training
for iter in range(60):
    layer_2_error = 0
    for i in range(len(streetlights)):
        layer_0 = streetlights[i:i+1]
        layer_1 = relu(np.dot(layer_0,weights_0_1))
        layer_2 = np.dot(layer_1,weights_1_2)

        layer_2_error += np.sum((layer_2 - walk_vs_stop[i]) ** 2)

        layer_2_delta = walk_vs_stop[i:i+1] - layer_2
        # 关键步骤
        layer_1_delta = layer_2_delta.dot(weights_1_2.T)*relu2deriv(layer_1)

        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)
        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)
    if(iter % 10 == 9):
        print("Error:" + str(layer_2_error))
```

```shell
# Analyse
1.关键
 1.设计负权重关闭的函数
 2.上一层的权重增量为权重乘以输出增量：layer_1_delta = layer_2_delta.dot(weights_1_2.T)*relu2deriv(layer_1)
 3.当前层的权重变化：当前层的权重+alpha*（当前层的input值*下一层的delta）
```

