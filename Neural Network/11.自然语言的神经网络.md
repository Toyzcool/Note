# 11.自然语言的神经网络

- NLP的任务
  - 标记文本区域。<!--比如词性标注、情感分类或命名实体识别-->
  - 链接两个以上的文本区域。<!--比如识别表示同一个实体的名词短语或代词，并归类-->
  - 填补基于上下文的信息空缺。<!--比如完形填空形式的补齐单词-->
- 主要算法
  - 词向量算法
  - 递归神经网络

## 11.3 监督NLP学习

- Purpose：神经网络旨在寻找输入和输出层之间的相关性。

### One-hot编码

- Rules

  - 行向量表示每一条语句。
  - 列向量表示语句中是否含有特定单词。
  - 如果一条语句中重复性出现单词，可以求和也可以只取一次结果。

- Example

  ![](assets/11_1.png)

  ```
  1.假设词汇表中有四个单词，分别是"cat","dog","the","sat"。
  2.如果有相应的单词，则将在特定位置的值为1，1的所在位置就是这个单词的索引。有多少个单词，就有多少列，因为每一个单词的1的所在位置不同。
  3.如果一条语句是"the cat sat"，则对应的语句向量就是"[1101]"。
  4.如果一条语句是"the cat sat cat",则对应的语句向量就是"[1101]"，或者求和就是"[2101]"。
  ```

- Implementation

  <!--简单实现-->
  
  ```python
  import numpy as np
  onehots = {}
  # 创建词袋
  onehots['cat'] = np.array([1,0,0,0])
  onehots['the'] = np.array([0,1,0,0])
  onehots['dog'] = np.array([0,0,1,0])
  onehots['sat'] = np.array([0,0,0,1])
  
  # 创建语句
  sentence = ['the','cat','sat']
  x = onehots[sentence[0]] + onehots[sentence[1]] + onehots[sentence[2]]
  
  # 输出结果
print('Sent Encoding:' + str(x))
  ```
  
  ```shell
  # output
Sent Encoding:[1 1 0 1]
  ```
  
  ```shell
  # Analyse
  1.创建词袋，每个单词都用向量表示。
  2.创建语句，分割语句中每个词，被分割的词作为关键词在词袋中检索，找到向量。
3.求向量和。
  ```
  

#### Example：电影评论

- Purpose：电影评论和具体评分是一对输入和输出，如何根据电影评论预测评分？

- Idea
  1. 评分是数字，因此作为输出需要的处理比较简单，因为原范围是1-5之间，现在需要缩小到0-1之间，这样可以应用softMax激活函数；电影评论是文字，然而文字中每个字母分析不合理（因为无法表达情绪或倾向），通过单词分析合理（比如bad），能够表达情绪或倾向。
  2. 评论是语句，所以使用one-hot编码来表示一句评论。
  3. 通过所有评论的单词，去重之后制作词袋。然后通过词袋中单词的索引来表示每一句评论语句（神经网络计算支持的格式）。
  
- Implementation

  <!--读取评价和评分，制作词袋-->

  ```python
  import sys
  
  # 1.读取评价
  f = open("reviews.txt")
  raw_reviews = f.readlines()
  f.close()
  
  # 2.读取评分
  f = open("labels.txt")
  raw_labels = f.readlines()
  f.close()
  
  
  # 3.function:将评价语句截成单词
  # note:使用set的原因是去除重复的单词
  def separate(x):
      return set(x.split(" "))
  
  
  # 4.将评价语句截成单词，用于制作词袋
  # note:map的结构是第一个参数是函数方法，第二个参数会作为值传入函数方法；list目的是将分开后的结果变成基本数据格式，类似Java数组。
  tokens = list(map(separate, raw_reviews))  # 每句话截成了单词，但依旧在list中
  print("Tokens:" + str(tokens))
  print("Tokens[0]:" + str(tokens[0]))
  print("================================")
  
  vocab = set()  # 创建一个空的set，合并去重
  for sent in tokens:  # 遍历list中的每个值
      for word in sent:  # 遍历每个值中的每个单词
          if len(word) > 0:  # 如何单词长度大于0，则加入空的set中
              vocab.add(word)
  vocab = list(vocab)  # 将set格式转换为list格式
  print("Vocab 所有非重复性词汇:" + str(vocab))
  print("================================")
  
  # 5.构建词袋
  # note:由于list中索引和单词对应，反过来就是one-hot编码形式的雏形
  word2index = {}
  for i,word in enumerate(vocab):
      word2index[word] = i
  print("Word2index 词袋:" + str(word2index))
  print("================================")
  
  
  # 6.创建评价语句的向量
  input_dataset = list()
  for sent in tokens:
      sent_indices = list()
      for word in sent:
          try:
              sent_indices.append(word2index[word])
          except:
              ""
      input_dataset.append(list(set(sent_indices)))	# 使用set，如果有句子在数字情况下一样，能够去重
  print("Input_dataset 评价语句的向量:" + str(input_dataset))
  print("================================")
  
  # 7.创建保存评分的list，1代表positive
  target_dataset = list()
  for label in raw_labels:
      if label == 'positive\n':
          target_dataset.append(1)
      else:
          target_dataset.append(0)
print("Target_dataset 所有评分:" + str(target_dataset))
  ```
  
  ```shell
  # output
  Tokens:[{'movie.\n', 'a', 'it', 'good', 'is'}, {'movies.\n', 'are', 'good', 'those'}]
  Tokens[0]:{'movie.\n', 'a', 'it', 'good', 'is'}
  ================================
  Vocab 所有非重复性词汇:['those', 'movies.\n', 'movie.\n', 'a', 'it', 'are', 'good', 'is']
  ================================
  Word2index 词袋:{'those': 0, 'movies.\n': 1, 'movie.\n': 2, 'a': 3, 'it': 4, 'are': 5, 'good': 6, 'is': 7}
  ================================
  Input_dataset 评价语句的向量:[[2, 3, 4, 6, 7], [0, 1, 5, 6]]
  ================================
Target_dataset 所有评分:[1, 0]
  ```
  
  ```shell
  总步骤
  1.制作词袋的步骤
  	1.分割所有的语句，生成list1，每个语句都是list1中的一项。
  	2.合并list1中的所有语句，生成set1，里面是所有语句中的单词并去重。
  	3.使用key和value来遍历set1，保存key和value（key是单词，value是原数字在list中的索引），生成词袋。
  2.将原来的语句通过词袋来生成语句向量，也就是神经网络计算的格式。
3.将评分转换成数字。
  ```
  

### 嵌入层

- Definition

  神经网络的网络结构是：第一层数据集（layer_0），然后线性层（weights_0_1）和relu层（layer_1），然后线性层（weights_1_2），然后输出层（layer_2）。由于词汇数量有几万，意味着layer_0有几万个1和0，当layer_0和weights_0_1进行乘积运算，相当于weights_0_1根据layer_0中是1的那几行（每个单词对应的行）进行和运算。选择矩阵中的对应行并求和（或平均值）的过程，也就是相当于将线性层（weights_0_1）看作嵌入层。
  
- Implementation

  <!--预测电影的完整代码-->

  ```python
  import sys
  import numpy as np
  
  ## 1.Preparation
  # 1.1 Read reviews and labels
  f = open("reviews.txt")
  raw_reviews = f.readlines()
  f.close()
  f = open("labels.txt")
  raw_labels = f.readlines()
  f.close()
  
  # 1.2 Create dictionary
  # Separate sentences
  tokens = list(map(lambda x: set(x.split(" ")), raw_reviews))
  vocab = set()
  for sent in tokens:
      for word in sent:
          if len(word) > 0:
              vocab.add(word)
  vocab = list(vocab)
  print("len of vocab:" + str(len(vocab)))
  # Create dictionary
  word2index = {}
  for i, word in enumerate(vocab):
      word2index[word] = i
  
  # 1.3 Transfer reviews via dictionary
  input_dataset = list()
  for sent in tokens:
      sent_indices = list()
      for word in sent:
          try:
              sent_indices.append(word2index[word])
          except:
              ""
      input_dataset.append(list(set(sent_indices)))
  print("len of input_dataset:" + str(len(input_dataset)))
  
  # 1.4 Transfer labels
  target_labels = list()
  for label in raw_labels:
      if label == 'positive\n':
          target_labels.append(1)
      else:
          target_labels.append(0)
  
  ## 2.Training
  np.random.seed(1)
  
  
  # 2.1 Basic parameters and functions
  # Activation function
  def sigmoid(x):
      return 1 / (1 + np.exp(-x))
  
  
  # Parameters
  alpha, iteration = (0.01, 2)
  hidden_size = 100
  
  weights_0_1 = 0.2 * np.random.random((len(vocab), hidden_size)) - 0.1  # [74074,100]
  weights_1_2 = 0.2 * np.random.random((hidden_size, 1)) - 0.1  # [100,1]
  
  correct, total = (0, 0)  # Calculate correct rate
  
  # 2.2 Training
  for iter in range(iteration):
  
      for i in range(len(input_dataset) - 1000):  # Train within 24000 reviews
  
          x, y = (input_dataset[i], target_labels[i])
          layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0))  # (100,)
          layer_2 = sigmoid(np.dot(layer_1, weights_1_2))  # (100,) * (100,1) = (1,)
  
          layer_2_delta = layer_2 - y
          layer_1_delta = layer_2_delta.dot(weights_1_2.T)  # (1,)* ((100,1).T) = (100,)
  
          weights_0_1[x] -= layer_1_delta * alpha
          weights_1_2 -= np.outer(layer_1, layer_2_delta) * alpha
  
          if np.abs(layer_2_delta) < 0.5:
              correct += 1
          total += 1
          if i % 10 == 9:
              progress = str(i / float(len(input_dataset)))
              sys.stdout.write('\rIter:' + str(iter) + ' Progress:' + progress[2:4] + '.' + progress[4:6] + '% Training Accuracy:' + str(correct / float(total)) + '%')
      print()
  ## 3.Test
  correct, total = (0, 0)
  for i in range(len(input_dataset) - 1000, len(input_dataset)):
      x = input_dataset[i]
      y = target_labels[i]
      layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0))
      layer_2 = sigmoid(np.dot(layer_1, weights_1_2))
      if np.abs(layer_2 - y) < 0.5:
          correct += 1
      total += 1
  print("Test Accuracy:" + str(correct / float(total)))
  ```

  ```shell
  # output
  len of vocab:74074
  len of input_dataset:25000
  Iter:0 Progress:95.99% Training Accuracy:0.83%
  Iter:1 Progress:95.99% Training Accuracy:0.8652916666666667%
  Test Accuracy:0.843
  ```

  ```shell
  # Analyse
  1.关键步骤是"layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0))"，将weights_0_1作为嵌入层之后，layer_1的计算就是对应行相加。axis = 0代表是垂直方向的相加，得到(100,)数组，也就是[1*100]的向量。
  ```

### 小结

- 神经网络寻找输入和输出集之间的相关性。
- 隐藏层意义是对上一层的数据进行分组。
- 如果两个数据点同时属于多个分组，则两个数据点相似；如果两个输入连接到各个隐藏层的权重相似，则两个输入相似。

### 欧氏距离（Euclidean Distance）

- Definition：比较每个条目到隐藏层神经元的权重，这种比较方法称为欧氏距离。

- Usage

  - 通过神经网络查到与目标单词最相似的单词。

- Principle：每个单词的向量与目标单词向量比较，到每个隐藏层神经元的权重大小，求差距最小的值，即为相似的值。

- Implementation

  ```python
  # 4.Function for Euclidean Distance
  def similar(target='terrible'):
      target_index = word2index[target]
      scores = Counter()
      for word, index in word2index.items():
          raw_difference = weights_0_1[index] - (weights_0_1[target_index])  # (100,)
          squared_difference = raw_difference * raw_difference    # (100,) * (100,) = (100,)
          scores[word] = -math.sqrt(sum(squared_difference))  # 求标准差
      return scores.most_common(10)
  print(similar())
  ```

  ```shell
  # output
  len of vocab:74074
  len of input_dataset:25000
  Iter:0 Progress:95.99% Training Accuracy:0.8334166666666667%
  Iter:1 Progress:95.99% Training Accuracy:0.8663333333333333%
  Test Accuracy:0.85
  [('terrible', -0.0), ('annoying', -0.7932396390575799), ('fails', -0.798839607084677), ('poor', -0.8114133039803418), ('mess', -0.844447497259539), ('horrible', -0.844878306793313), ('dull', -0.8458943998490457), ('lame', -0.8480644537284596), ('avoid', -0.852650392755622), ('boring', -0.8600860550633572)]
  ```

  ```python
  import numpy as np
  import sys, math
  from collections import Counter
  
  ## 1.Preparation
  # 1.1 Read reviews and labels
  f = open("reviews.txt")
  raw_reviews = f.readlines()
  f.close()
  f = open("labels.txt")
  raw_labels = f.readlines()
  f.close()
  
  # 1.2 Create dictionary
  # Separate sentences
  tokens = list(map(lambda x: set(x.split(" ")), raw_reviews))
  vocab = set()
  for sent in tokens:
      for word in sent:
          if len(word) > 0:
              vocab.add(word)
  vocab = list(vocab)
  print("len of vocab:" + str(len(vocab)))
  # Create dictionary
  word2index = {}
  for i, word in enumerate(vocab):
      word2index[word] = i
  
  # 1.3 Transfer reviews via dictionary
  input_dataset = list()
  for sent in tokens:
      sent_indices = list()
      for word in sent:
          try:
              sent_indices.append(word2index[word])
          except:
              ""
      input_dataset.append(list(set(sent_indices)))
  print("len of input_dataset:" + str(len(input_dataset)))
  
  # 1.4 Transfer labels
  target_labels = list()
  for label in raw_labels:
      if label == 'positive\n':
          target_labels.append(1)
      else:
          target_labels.append(0)
  
  ## 2.Training
  np.random.seed(1)
  
  
  # 2.1 Basic parameters and functions
  # Activation function
  def sigmoid(x):
      return 1 / (1 + np.exp(-x))
  
  
  # Parameters
  alpha, iteration = (0.01, 2)
  hidden_size = 100
  
  weights_0_1 = 0.2 * np.random.random((len(vocab), hidden_size)) - 0.1  # [74074,100]
  weights_1_2 = 0.2 * np.random.random((hidden_size, 1)) - 0.1  # [100,1]
  
  correct, total = (0, 0)  # Calculate correct rate
  
  # 2.2 Training
  for iter in range(iteration):
  
      for i in range(len(input_dataset) - 1000):  # Train within 24000 reviews
  
          x, y = (input_dataset[i], target_labels[i])
          layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0))  # [1,100]; Because axis is 0, each columns will add.
          layer_2 = sigmoid(np.dot(layer_1, weights_1_2))  # [1,100] * [100,1] = [1]
  
          layer_2_delta = layer_2 - y
          layer_1_delta = layer_2_delta.dot(weights_1_2.T)
  
          weights_0_1[x] -= layer_1_delta * alpha
          weights_1_2 -= np.outer(layer_1, layer_2_delta) * alpha
  
          if np.abs(layer_2_delta) < 0.5:
              correct += 1
          total += 1
          if i % 10 == 9:
              progress = str(i / float(len(input_dataset)))
              sys.stdout.write('\rIter:' + str(iter) + ' Progress:' + progress[2:4] + '.' + progress[4:6] + '% Training Accuracy:' + str(correct / float(total)) + '%')
      print()
  
  ## 3.Test
  correct, total = (0, 0)
  for i in range(len(input_dataset) - 1000, len(input_dataset)):
      x = input_dataset[i]
      y = target_labels[i]
      layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0))
      layer_2 = sigmoid(np.dot(layer_1, weights_1_2))
      if np.abs(layer_2 - y) < 0.5:
          correct += 1
      total += 1
  print("Test Accuracy:" + str(correct / float(total)))
  
  
  ```

  

