> fix

# 11.自然语言的神经网络

- NLP的任务
  - 标记文本区域。<!--比如词性标注、情感分类或命名实体识别-->
  - 链接两个以上的文本区域。<!--比如识别表示同一个实体的名词短语或代词，并归类-->
  - 填补基于上下文的信息空缺。<!--比如完形填空形式的补齐单词-->
- 主要算法
  - 词向量算法
  - 递归神经网络

## 11.3 监督NLP学习

- Purpose：神经网络旨在寻找输入和输出层之间的相关性。

### 词嵌入

- Definition：把维数是所有词的高维空间嵌入到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量。

  比如单词cat在词袋中对应的位置值所对应的权重矩阵的那一列。

  <!--一个单词在算法中的存储形式-->

- Application

  - 单词类比。通过比较词嵌入的向量各个点的差距计算欧氏距离来寻找近义词。

- Method：Word2vec，GloVe

- Principle：基于词义分布理论 distributional hypothesis 来构建的（即某些词如果出现于相同上下文中则它们可能具有相同或类似的意义）

### One-hot编码

- Rules

  - 行向量表示每一条语句。
  - 列向量表示语句中是否含有特定单词。
  - 如果一条语句中重复性出现单词，可以求和也可以只取一次结果。

- Disadvantage 

  - 每个单词彼此独立，实际情况是部分单词有较强的相关性。比如buy和bought。

- Example

  ![](assets/11_1.png)

  ```
  1.假设词汇表中有四个单词，分别是"cat","dog","the","sat"。
  2.如果有相应的单词，则将在特定位置的值为1，1的所在位置就是这个单词的索引。有多少个单词，就有多少列，因为每一个单词的1的所在位置不同。
  3.如果一条语句是"the cat sat"，则对应的语句向量就是"[1101]"。
  4.如果一条语句是"the cat sat cat",则对应的语句向量就是"[1101]"，或者求和就是"[2101]"。
  ```

- Implementation

  <!--简单实现-->
  
  ```python
  import numpy as np
  onehots = {}
  # 创建词袋
  onehots['cat'] = np.array([1,0,0,0])
  onehots['the'] = np.array([0,1,0,0])
  onehots['dog'] = np.array([0,0,1,0])
  onehots['sat'] = np.array([0,0,0,1])
  
  # 创建语句
  sentence = ['the','cat','sat']
  x = onehots[sentence[0]] + onehots[sentence[1]] + onehots[sentence[2]]
  
  # 输出结果
print('Sent Encoding:' + str(x))
  ```
  
  ```shell
  # output
Sent Encoding:[1 1 0 1]
  ```
  
  ```shell
  # Analyse
  1.创建词袋，每个单词都用向量表示。
  2.创建语句，分割语句中每个词，被分割的词作为关键词在词袋中检索，找到向量。
3.求向量和。
  ```
  

#### Example：电影评论

- Purpose：电影评论和具体评分是一对输入和输出，如何根据电影评论预测评分？

- Idea
  1. 评分是数字，因此作为输出需要的处理比较简单，因为原范围是1-5之间，现在需要缩小到0-1之间，这样可以应用softMax激活函数；电影评论是文字，然而文字中每个字母分析不合理（因为无法表达情绪或倾向），通过单词分析合理（比如bad），能够表达情绪或倾向。
  2. 评论是语句，所以使用one-hot编码来表示一句评论。
  3. 通过所有评论的单词，去重之后制作词袋。然后通过词袋中单词的索引来表示每一句评论语句（神经网络计算支持的格式）。
  
- Implementation

  <!--读取评价和评分，制作词袋-->

  ```python
  import sys
  
  # 1.读取评价
  f = open("reviews.txt")
  raw_reviews = f.readlines()
  f.close()
  
  # 2.读取评分
  f = open("labels.txt")
  raw_labels = f.readlines()
  f.close()
  
  
  # 3.function:将评价语句截成单词
  # note:使用set的原因是去除重复的单词
  def separate(x):
      return set(x.split(" "))
  
  
  # 4.将评价语句截成单词，用于制作词袋
  # note:map的结构是第一个参数是函数方法，第二个参数会作为值传入函数方法；list目的是将分开后的结果变成基本数据格式，类似Java数组。
  tokens = list(map(separate, raw_reviews))  # 每句话截成了单词，但依旧在list中
  print("Tokens:" + str(tokens))
  print("Tokens[0]:" + str(tokens[0]))
  print("================================")
  
  vocab = set()  # 创建一个空的set，合并去重
  for sent in tokens:  # 遍历list中的每个值
      for word in sent:  # 遍历每个值中的每个单词
          if len(word) > 0:  # 如何单词长度大于0，则加入空的set中
              vocab.add(word)
  vocab = list(vocab)  # 将set格式转换为list格式
  print("Vocab 所有非重复性词汇:" + str(vocab))
  print("================================")
  
  # 5.构建词袋
  # note:由于list中索引和单词对应，反过来就是one-hot编码形式的雏形
  word2index = {}
  for i,word in enumerate(vocab):
      word2index[word] = i
  print("Word2index 词袋:" + str(word2index))
  print("================================")
  
  
  # 6.创建评价语句的向量
  input_dataset = list()
  for sent in tokens:
      sent_indices = list()
      for word in sent:
          try:
              sent_indices.append(word2index[word])
          except:
              ""
      input_dataset.append(list(set(sent_indices)))	# 使用set，如果有句子在数字情况下一样，能够去重
  print("Input_dataset 评价语句的向量:" + str(input_dataset))
  print("================================")
  
  # 7.创建保存评分的list，1代表positive
  target_dataset = list()
  for label in raw_labels:
      if label == 'positive\n':
          target_dataset.append(1)
      else:
          target_dataset.append(0)
print("Target_dataset 所有评分:" + str(target_dataset))
  ```
  
  ```shell
  # output
  Tokens:[{'movie.\n', 'a', 'it', 'good', 'is'}, {'movies.\n', 'are', 'good', 'those'}]
  Tokens[0]:{'movie.\n', 'a', 'it', 'good', 'is'}
  ================================
  Vocab 所有非重复性词汇:['those', 'movies.\n', 'movie.\n', 'a', 'it', 'are', 'good', 'is']
  ================================
  Word2index 词袋:{'those': 0, 'movies.\n': 1, 'movie.\n': 2, 'a': 3, 'it': 4, 'are': 5, 'good': 6, 'is': 7}
  ================================
  Input_dataset 评价语句的向量:[[2, 3, 4, 6, 7], [0, 1, 5, 6]]
  ================================
Target_dataset 所有评分:[1, 0]
  ```
  
  ```shell
  总步骤
  1.制作词袋的步骤
  	1.分割所有的语句，生成list1，每个语句都是list1中的一项。
  	2.合并list1中的所有语句，生成set1，里面是所有语句中的单词并去重。
  	3.使用key和value来遍历set1，保存key和value（key是单词，value是原数字在list中的索引），生成词袋。
  2.将原来的语句通过词袋来生成语句向量，也就是神经网络计算的格式。
3.将评分转换成数字。
  ```
  

### 嵌入层

- Definition

  神经网络的网络结构是：第一层数据集（layer_0），然后线性层（weights_0_1）和relu层（layer_1），然后线性层（weights_1_2），然后输出层（layer_2）。由于词汇数量有几万，意味着layer_0有几万个1和0，当layer_0和weights_0_1进行乘积运算，相当于weights_0_1根据layer_0中是1的那几行（每个单词对应的行）进行和运算。选择矩阵中的对应行并求和（或平均值）的过程，也就是相当于将线性层（weights_0_1）看作嵌入层。
  
- Implementation

  <!--预测电影的完整代码-->

  ```python
  import sys
  import numpy as np
  
  ## 1.Preparation
  # 1.1 Read reviews and labels
  f = open("reviews.txt")
  raw_reviews = f.readlines()
  f.close()
  f = open("labels.txt")
  raw_labels = f.readlines()
  f.close()
  
  # 1.2 Create dictionary
  # Separate sentences
  tokens = list(map(lambda x: set(x.split(" ")), raw_reviews))
  vocab = set()
  for sent in tokens:
      for word in sent:
          if len(word) > 0:
              vocab.add(word)
  vocab = list(vocab)
  print("len of vocab:" + str(len(vocab)))
  # Create dictionary
  word2index = {}
  for i, word in enumerate(vocab):
      word2index[word] = i
  
  # 1.3 Transfer reviews via dictionary
  input_dataset = list()
  for sent in tokens:
      sent_indices = list()
      for word in sent:
          try:
              sent_indices.append(word2index[word])
          except:
              ""
      input_dataset.append(list(set(sent_indices)))
  print("len of input_dataset:" + str(len(input_dataset)))
  
  # 1.4 Transfer labels
  target_labels = list()
  for label in raw_labels:
      if label == 'positive\n':
          target_labels.append(1)
      else:
          target_labels.append(0)
  
  ## 2.Training
  np.random.seed(1)
  
  
  # 2.1 Basic parameters and functions
  # Activation function
  def sigmoid(x):
      return 1 / (1 + np.exp(-x))
  
  
  # Parameters
  alpha, iteration = (0.01, 2)
  hidden_size = 100
  
  weights_0_1 = 0.2 * np.random.random((len(vocab), hidden_size)) - 0.1  # [74074,100]
  weights_1_2 = 0.2 * np.random.random((hidden_size, 1)) - 0.1  # [100,1]
  
  correct, total = (0, 0)  # Calculate correct rate
  
  # 2.2 Training
  for iter in range(iteration):
  
      for i in range(len(input_dataset) - 1000):  # Train within 24000 reviews
  
          x, y = (input_dataset[i], target_labels[i])
          layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0))  # (100,)
          layer_2 = sigmoid(np.dot(layer_1, weights_1_2))  # (100,) * (100,1) = (1,)
  
          layer_2_delta = layer_2 - y
          layer_1_delta = layer_2_delta.dot(weights_1_2.T)  # (1,)* ((100,1).T) = (100,)
  
          weights_0_1[x] -= layer_1_delta * alpha
          weights_1_2 -= np.outer(layer_1, layer_2_delta) * alpha
  
          if np.abs(layer_2_delta) < 0.5:
              correct += 1
          total += 1
          if i % 10 == 9:
              progress = str(i / float(len(input_dataset)))
              sys.stdout.write('\rIter:' + str(iter) + ' Progress:' + progress[2:4] + '.' + progress[4:6] + '% Training Accuracy:' + str(correct / float(total)) + '%')
      print()
  ## 3.Test
  correct, total = (0, 0)
  for i in range(len(input_dataset) - 1000, len(input_dataset)):
      x = input_dataset[i]
      y = target_labels[i]
      layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0))
      layer_2 = sigmoid(np.dot(layer_1, weights_1_2))
      if np.abs(layer_2 - y) < 0.5:
          correct += 1
      total += 1
  print("Test Accuracy:" + str(correct / float(total)))
  ```

  ```shell
  # output
  len of vocab:74074
  len of input_dataset:25000
  Iter:0 Progress:95.99% Training Accuracy:0.83%
  Iter:1 Progress:95.99% Training Accuracy:0.8652916666666667%
  Test Accuracy:0.843
  ```

  ```shell
  # Analyse
  1.关键步骤是"layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0))"，将weights_0_1作为嵌入层之后，layer_1的计算就是对应行相加。axis = 0代表是垂直方向的相加，得到(100,)数组，也就是[1*100]的向量。
  ```

### 小结

- 神经网络寻找输入和输出集之间的相关性。
- 隐藏层意义是对上一层的数据进行分组。
- 如果两个数据点同时属于多个分组，则两个数据点相似；如果两个输入连接到各个隐藏层的权重相似，则两个输入相似。

### 欧氏距离（Euclidean Distance）

- Definition：比较每个条目到隐藏层神经元的权重，这种比较方法称为欧氏距离。

- Usage

  - 通过神经网络查到与目标单词最相似的单词。

- Principle：每个单词的向量与目标单词向量比较，到每个隐藏层神经元的权重大小，求差距最小的值，即为相似的值。

- Implementation

  ```python
  # 4.Function for Euclidean Distance
  def similar(target='terrible'):
      target_index = word2index[target]
      scores = Counter()
      for word, index in word2index.items():
          raw_difference = weights_0_1[index] - (weights_0_1[target_index])  # (100,)
          squared_difference = raw_difference * raw_difference    # (100,) * (100,) = (100,)
          scores[word] = -math.sqrt(sum(squared_difference))  # 求标准差
      return scores.most_common(10)
  print(similar())
  ```

  ```shell
  # output
  [('terrible', -0.0), ('annoying', -0.7932396390575799), ('fails', -0.798839607084677), ('poor', -0.8114133039803418), ('mess', -0.844447497259539), ('horrible', -0.844878306793313), ('dull', -0.8458943998490457), ('lame', -0.8480644537284596), ('avoid', -0.852650392755622), ('boring', -0.8600860550633572)]
  ```
  
  ```python
  import numpy as np
  import sys, math
  from collections import Counter

  ## 1.Preparation
  # 1.1 Read reviews and labels
  f = open("reviews.txt")
  raw_reviews = f.readlines()
  f.close()
  f = open("labels.txt")
  raw_labels = f.readlines()
  f.close()
  
  # 1.2 Create dictionary
  # Separate sentences
  tokens = list(map(lambda x: set(x.split(" ")), raw_reviews))
  vocab = set()
  for sent in tokens:
      for word in sent:
          if len(word) > 0:
              vocab.add(word)
  vocab = list(vocab)
  print("len of vocab:" + str(len(vocab)))
  # Create dictionary
  word2index = {}
  for i, word in enumerate(vocab):
      word2index[word] = i
  
  # 1.3 Transfer reviews via dictionary
  input_dataset = list()
  for sent in tokens:
      sent_indices = list()
      for word in sent:
          try:
              sent_indices.append(word2index[word])
          except:
              ""
      input_dataset.append(list(set(sent_indices)))
  print("len of input_dataset:" + str(len(input_dataset)))
  
  # 1.4 Transfer labels
  target_labels = list()
  for label in raw_labels:
      if label == 'positive\n':
          target_labels.append(1)
      else:
          target_labels.append(0)
  
  ## 2.Training
  np.random.seed(1)
  
  
  # 2.1 Basic parameters and functions
  # Activation function
  def sigmoid(x):
      return 1 / (1 + np.exp(-x))
  
  
  # Parameters
  alpha, iteration = (0.01, 2)
  hidden_size = 100
  
  weights_0_1 = 0.2 * np.random.random((len(vocab), hidden_size)) - 0.1  # [74074,100]
  weights_1_2 = 0.2 * np.random.random((hidden_size, 1)) - 0.1  # [100,1]
  
  correct, total = (0, 0)  # Calculate correct rate
  
  # 2.2 Training
  for iter in range(iteration):
  
      for i in range(len(input_dataset) - 1000):  # Train within 24000 reviews
  
          x, y = (input_dataset[i], target_labels[i])
          layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0))  # [1,100]; Because axis is 0, each columns will add.
          layer_2 = sigmoid(np.dot(layer_1, weights_1_2))  # [1,100] * [100,1] = [1]
  
          layer_2_delta = layer_2 - y
          layer_1_delta = layer_2_delta.dot(weights_1_2.T)
  
          weights_0_1[x] -= layer_1_delta * alpha
          weights_1_2 -= np.outer(layer_1, layer_2_delta) * alpha
  
          if np.abs(layer_2_delta) < 0.5:
              correct += 1
          total += 1
          if i % 10 == 9:
              progress = str(i / float(len(input_dataset)))
              sys.stdout.write('\rIter:' + str(iter) + ' Progress:' + progress[2:4] + '.' + progress[4:6] + '% Training Accuracy:' + str(correct / float(total)) + '%')
      print()
  
  ## 3.Test
  correct, total = (0, 0)
  for i in range(len(input_dataset) - 1000, len(input_dataset)):
      x = input_dataset[i]
      y = target_labels[i]
      layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0))
      layer_2 = sigmoid(np.dot(layer_1, weights_1_2))
      if np.abs(layer_2 - y) < 0.5:
          correct += 1
      total += 1
  print("Test Accuracy:" + str(correct / float(total)))
  ```
  

### 完形填空——Important!!!

- Requirements：将文本划分成5个单词一组的短语，删除一个重点词汇，尝试训练网络，利用去掉单词之后的剩余部分来预测去掉的单词。

- Implementation

  ```python
  import numpy as np
  import sys, math, random
  from collections import Counter
  
  ## Preparation
  np.random.seed(1)
  random.seed(1)
  f = open("reviews.txt")
  raw_reviews = f.readlines()
  f.close()
  
  tokens = list(map(lambda x: x.split(" "), raw_reviews))  
  wordcnt = Counter()
  for sent in tokens:
      for word in sent:
          wordcnt[word] -= 1  
          # 将每个单词出现一次就减去1次，到最后出现次数越多，数字越小。因为wordcnt中没有单词，所以每个单词的初始次数都是0。比如单词"can"，初始值为0，出现一次后就为-1，再次出现，则为-2，即使是在不同的sent中出现。
  vocab = list(set(map(lambda x: x[0], wordcnt.most_common())))
  # 因为返回值是x[0],所以返回的是单词，也就是将所有的单词加入vocab中。如果是x[1]，则返回的是统计次数。
  
  word2index = {}
  for i, word in enumerate(vocab):
      word2index[word] = i
  
  concatenated = list()
  input_dataset = list()
  for sent in tokens:
      sent_indices = list()
      for word in sent:
          try:
              sent_indices.append(word2index[word])
              concatenated.append(word2index[word])
              # 所有的单词的索引都放在一个list中,一共有7459318个词
          except:
              ""
      input_dataset.append(sent_indices)
  concatenated = np.array(concatenated)  # 将list转换为array;(7459318,)
  random.shuffle(input_dataset)  
  # 随机打乱input_dataset中的数据顺序。比如二维数组，只打乱第一维的顺序。
  
  ## Basic parameters and methods
  alpha, iterations = (0.05, 2)
  hidden_size, window, negative = (50, 2, 5)  
  # negative是5，因为自定义5个单词为一组。window为2，用途是选择被预测单词的前后各两项。
  
  weights_0_1 = (np.random.rand(len(vocab), hidden_size) - 0.5) * 0.2  # (74075,50)
  weights_1_2 = np.random.rand(len(vocab), hidden_size) * 0  # (74075,50)，元素都为零
  
  layer_2_target = np.zeros(negative + 1)  # [0. 0. 0. 0. 0. 0.]
  layer_2_target[0] = 1  # [1. 0. 0. 0. 0. 0.]
  # 因为被预测的单词的位置都是放在target_samples的第一位，所以对应的目标结果（layer_2_target）的第一位就是1，表示答案就是layer_2_target！
  
  
  # 输出10个beautiful的近似词；该方法用来查看在完成完形填空之后，查找近义词的变化。
  def similar(target="beautiful"):
      target_index = word2index[target]
  
      scores = Counter()
      for word, index in word2index.items():
          raw_difference = weights_0_1[index] - weights_0_1[target_index]
          squared_difference = raw_difference * raw_difference
          scores[word] = -math.sqrt(sum(squared_difference))
      return scores.most_common(10)
  
  
  def sigmoid(x):
      return 1 / (1 + np.exp(-x))
  
  
  # Training
  for rev_i, review in enumerate(input_dataset * iterations):
  # rev_i用来表示每条review的索引；input_dataset * iterations的值是两个input_dataset的list
      for target_i in range(len(review)):
      # target_i表示被预测的单词的索引!
          target_samples = [review[target_i]]+list(concatenated[(np.random.rand(negative) * len(concatenated)).astype('int').tolist()])
        # 见Analyse
          left_context = review[max(0, target_i - window):target_i]   
          # 取索引是target_i的左边相邻的单词。
          right_context = review[target_i + 1:min(len(review), target_i + window)]    
          # 取索引是target_i的右边相邻的单词；如果target_i是5，则左边取3个单词，右边取2个单词。
          
          layer_1 = np.mean(weights_0_1[left_context + right_context], axis=0)    
          # 取被预测单词的左边右边共五个元素所对应的weights_0_1权重，然后垂直相加，作为输入层。因为layer_0是1和0的向量，和权重矩阵相乘，就是矩阵对应列的值做加法。 (50,)
          layer_2 = sigmoid(layer_1.dot(weights_1_2[target_samples].T))
          # 见Analyse
          
          layer_2_delta = layer_2 - layer_2_target
          layer_1_delta = layer_2_delta.dot(weights_1_2[target_samples])
  
          weights_0_1[left_context + right_context] -= layer_1_delta * alpha
          weights_1_2[target_samples] -= np.outer(layer_2_delta, layer_1) * alpha
  
      if rev_i % 250 == 0:
          sys.stdout.write(
              '\rProgress:' + str(rev_i / float(len(input_dataset) * iterations)) + "   " + str(
                  similar('terrible')))
          sys.stdout.write('\rProgress:' + str(rev_i / float(len(input_dataset) * iterations)))
  
  print(similar('terrible'))
  ```

  ```shell
  # Analyse
  1. "target_samples = [review[target_i]]+list(concatenated[(np.random.rand(negative) * len(concatenated)).astype('int').tolist()])"
  	1."target_samples"是为了每次只预测一个随机子集。如果对每一个单词都进行预测，代价过高。比如为了预测缺少的是哪个单词，需要对每一个可能的单词添加标签，代价过大，因此每次正向传播都随机忽略大多数标签，就预测一个子集。
  	2."[review[target_i]]"就是取到了删去的单词。之所以放在target_samples的第一位，就是对应了layer_2_target[0]的位置。
  	3."np.random.rand(negative) * len(concatenated)"是为了能够覆盖到所有评论的单词，并选出5个。"astype('int')"是将五个值都变成int类型，作为索引，在concatenated中取值。
  	4.最后将选中的值和随机的五个值组合成一个list()
  2."layer_2 = sigmoid(layer_1.dot(weights_1_2[target_samples].T))"
  	1."weights_1_2"是零的矩阵，因此"layer_1.dot(weights_1_2[target_samples].T)"的结果在第一次迭代时为零，经过"sigmoid()"激活函数后是0.5的向量。
  	2.[1,50] * [50,6] = [1,6] 也就是(6,)
  	3."weights_1_2[target_samples]"中是"target_samples"的原因是，你预测的目标是"target_samples",所以乘以对应的权重。比如layer_0求layer_1的时候，weights_0_1的值就是layer_1所对应的列！！！
  ```

### 小结

- 电影评论和完形填空比较
  - 预测电影评论和完形填空之后，查找beautiful的近义词的结果不相同。电影评论时，单词将根据预测是正面还是负面的标签的可能性聚集在一起。完形填空将基于同一个短语中出现的可能性聚集在一起。也称为“智能定位”。

## 11.13 Loss Function

- Formula

  <!--目前只使用这一种损失函数。逻辑回归的损失函数可以见讲义-->

  ```
  error = ((0.5 * weights) - 0.8) ** 2
  ```

- Rules

  - 误差函数的不同，将导致每个网络中单词聚集的方式不同。

    <!--比如电影评论和完形填空，两个网络架构相似，数据集相同，由于学习目标值的不同，导致误差函数不同，导致单词聚集的方式也不同。-->

> 学习的过程就是建立一个损失函数，然后将它最小化。

## 11.15 单词类比

- Example

  ```python
  king = [0.6, 0.1]
  man = [0.5, 0.0]
  woman = [0.0, 0.8]
  queen = [0.1, 1.0]
  
  king - man = [0.1, 0.1]
  queen - woman = [0.1, 0.2]
  ```

  预测结果就是king，man；queen，woman之间的关系是相似的。

- Implementation

  ```python
  def analogy(position = ['terrible','good'],negative = ['bad']):
    
    norms = np.sum(weights_0_1 * weights_0_1, axis = 1)
    norms.resize(norms.shape[0],1)
    
    normed_weights = weights_0_1 * norms
    
    query_vect = np.zeros(len(weights_0_1[0]))
    for word in positive:
      query_vect += normed_weights[word2index[word]]
    for word in negative:
      query_vect -= normed_weights[word2index[word]]
      
    scores = Counter()
    for word, index in word2index.items():
      raw_difference = weights_0_1[index] - query_vect
      squared_difference = raw_difference * raw_difference
      scores[word] = -math.sqrt(sum(squared_difference))
    
    return scores.most_common(10)[1:]
  ```